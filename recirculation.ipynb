{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Learning Representations by Recirculation"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from typing import Union\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%` not found.\n"
     ]
    }
   ],
   "source": [
    "% matplotlib notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Activation Functions\n",
    "In the paper the only activation function described is the logistic function. They said that other smooth monotonic functions will work too. Here I implemented the logistic function, and also other smooth monotonic functions.\n",
    "* Logistic: $$f(x)=\\frac{1}{1+e^{-x}}$$\n",
    "* Linear:$$f(x)=x$$\n",
    "* Tanh: $$f(x)=\\frac{e^{2x}-1}{e^{2x}+1}$$\n",
    "* Rectified Linear Unit (ReLU): $$f(x)=x^+=max(0,x)$$\n",
    "* Leaky Rectified Linear Unit (Leak ReLU):\n",
    "$$\n",
    "f(x)=\n",
    "    \\begin{cases}\n",
    "    x  &\\quad \\text{if } x>0\\\\\n",
    "    ax & \\quad \\text{if } x<0\n",
    "    \\end{cases}\n",
    "$$"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def logistic(x: np.ndarray):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "\n",
    "def linear(x: np.ndarray):\n",
    "    return x\n",
    "\n",
    "\n",
    "def tanh(x: np.ndarray):\n",
    "    return (np.exp(2 * x) - 1) / (np.exp(2 * x) + 1)\n",
    "\n",
    "\n",
    "def relu(x: np.ndarray):\n",
    "    return np.maximum(x, 0)\n",
    "\n",
    "\n",
    "def leaky_relu(x: np.ndarray, a: float = 0.01):\n",
    "    return np.where(x > 0, x, x * a)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Weight Initializers"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def uniform_initializer(low, high, shape):\n",
    "    return np.random.uniform(low, high, shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Error Metrics"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def squared_reconstruction_error(y1, y2):\n",
    "    return 1 / 2 * np.sum((y1 - y2) ** 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Util Functions"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def add_bias(x):\n",
    "    shape = list(x.shape)\n",
    "    shape[-1] = 1\n",
    "    bias = np.ones(shape)\n",
    "    return np.hstack((x, bias))"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Recirculation Implementations"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class Recirculation:\n",
    "    ACTIVATIONS = {\n",
    "        'logistic': logistic\n",
    "    }\n",
    "\n",
    "    INITIALIZERS = {\n",
    "        'uniform': uniform_initializer\n",
    "    }\n",
    "\n",
    "    def __init__(self, input_len: int, hidden_units: Union[int, list], regression_rate: float, learning_rate: float,\n",
    "                 activation_function: str, weight_initializer: str, low_weight_range: float = -0.5,\n",
    "                 high_weight_range: float = 0.5):\n",
    "        if type(hidden_units) is int:\n",
    "            hidden_units = [hidden_units]\n",
    "\n",
    "        self.activation_function = self.ACTIVATIONS[activation_function]\n",
    "        self.weight_initializer = self.INITIALIZERS[weight_initializer]\n",
    "        self.regression_rate = regression_rate\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "        self.layer_shapes = [input_len] * 2\n",
    "        for unit in hidden_units:\n",
    "            self.layer_shapes.insert(-1, unit)\n",
    "\n",
    "        self.layer_count = len(self.layer_shapes)\n",
    "\n",
    "        self.weights = []\n",
    "        for i in range(self.layer_count - 1):\n",
    "            self.weights.append(\n",
    "                self.weight_initializer(\n",
    "                    low_weight_range, high_weight_range,\n",
    "                    # adding one to the first shape is because of the bias\n",
    "                    (self.layer_shapes[i] + 1, self.layer_shapes[i + 1])\n",
    "                )\n",
    "            )\n",
    "\n",
    "    def fit(self, x: np.ndarray, epoch: int = 100, loging: str = 'low') -> dict:\n",
    "        if loging not in ['low', 'high']:\n",
    "            raise ValueError('Incorrect loging value. It can be either low or high.')\n",
    "\n",
    "        history = {\n",
    "            'error': [],\n",
    "        }\n",
    "        if loging == 'high':\n",
    "            history['weights'] = [self.weights]\n",
    "            history['prediction_history'] = [self.predict(x)]\n",
    "            history['predicted_classes_history'] = [self.predict_class(x)]\n",
    "\n",
    "        for _ in range(epoch):\n",
    "            layer_outputs = [x]\n",
    "            # first pass\n",
    "            # calculate hidden layer outputs (based on equation 1, 2)\n",
    "            for i in range(self.layer_count - 2):\n",
    "                layer_outputs.append(\n",
    "                    self.activation_function(\n",
    "                        add_bias(layer_outputs[-1]) @ self.weights[i]\n",
    "                    )\n",
    "                )\n",
    "            # calculate output with regression (based on equation 5)\n",
    "            layer_outputs.append(\n",
    "                self.regression_rate * layer_outputs[0] + (1 - self.regression_rate) *\n",
    "                self.activation_function(\n",
    "                    add_bias(layer_outputs[-1]) @ self.weights[-1]\n",
    "                )\n",
    "            )\n",
    "            # note: the above statement can be inside the for loop below but this is more readable\n",
    "\n",
    "            # second pass\n",
    "            # calculate hidden layer outputs (based on equation 6)\n",
    "            for i in range(1, self.layer_count - 1):\n",
    "                layer_outputs.append(\n",
    "                    self.regression_rate * layer_outputs[i] + (1 - self.regression_rate) *\n",
    "                    self.activation_function(\n",
    "                        add_bias(layer_outputs[-1]) @ self.weights[i - 1]\n",
    "                    )\n",
    "                )\n",
    "\n",
    "            # compute reconstruction error\n",
    "            error = squared_reconstruction_error(layer_outputs[0], layer_outputs[self.layer_count - 1])\n",
    "\n",
    "            # updating weights (based on equations 3, 4)\n",
    "            for i in range(self.layer_count - 1):\n",
    "                delta_w = self.learning_rate * add_bias(layer_outputs[i + 1]).T @ (\n",
    "                        layer_outputs[i] - layer_outputs[i + 2])\n",
    "                self.weights[len(self.weights) - 1 - i] += delta_w\n",
    "\n",
    "            # update history\n",
    "            history['error'].append(error)\n",
    "            if loging == 'high':\n",
    "                history['weights'].append(self.weights)\n",
    "                history['prediction_history'].append(self.predict(x))\n",
    "                history['predicted_classes_history'].append(self.predict_class(x))\n",
    "\n",
    "        return history\n",
    "\n",
    "    def predict(self, x: np.ndarray) -> np.ndarray:\n",
    "        layer_output = [x]\n",
    "        for i in range(self.layer_count - 1):\n",
    "            layer_output.append(\n",
    "                self.activation_function(\n",
    "                    add_bias(layer_output[-1]) @ self.weights[i]\n",
    "                )\n",
    "            )\n",
    "        return layer_output[-1]\n",
    "\n",
    "    def predict_class(self, x: np.ndarray) -> np.ndarray:\n",
    "        pred = self.predict(x)\n",
    "        classes = np.zeros_like(pred)\n",
    "        classes[np.arange(len(classes)), pred.argmax(axis=1)] = 1\n",
    "        return classes\n",
    "\n",
    "    def encode(self, x):\n",
    "        layer_output = [x]\n",
    "        for i in range(self.layer_count - 2):\n",
    "            layer_output.append(\n",
    "                self.activation_function(\n",
    "                    add_bias(layer_output[-1]) @ self.weights[i]\n",
    "                )\n",
    "            )\n",
    "        return layer_output[-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "x = np.array([\n",
    "    [1, 0, 0, 0],\n",
    "    [0, 1, 0, 0],\n",
    "    [0, 0, 1, 0],\n",
    "    [0, 0, 0, 1],\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    'input_len': 4,\n",
    "    'hidden_units': 2,\n",
    "    'regression_rate': 0.75,\n",
    "    'learning_rate': 20,\n",
    "    'activation_function': 'logistic',\n",
    "    'weight_initializer': 'uniform',\n",
    "    'low_weight_range': -0.5,\n",
    "    'high_weight_range': 0.5,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "r = Recirculation(**parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "history = r.fit(x, loging='high')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(np.arange(100), np.array(history['error']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "view predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "r.predict_class(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "r.predict(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "r.encode(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Let's do the test for 100 times to see average number of weight updates to get an error bellow 0.05. The paper tested the average for the error bellow 0.1 but looking at the error graph above, we see that the model achieves 0.1 error in the first few epochs but because of the high regression rate, it's not stable. However, it is more stable around 0.05 error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "indexes = []\n",
    "for _ in range(100):\n",
    "    r = Recirculation(**parameters)\n",
    "    history = r.fit(x)\n",
    "    error_array = np.array(history['error'])\n",
    "    indexes.append(np.argmax(error_array < 0.05))\n",
    "print(np.average(indexes))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}